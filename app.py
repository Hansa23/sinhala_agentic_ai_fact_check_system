"""Streamlit app for Sinhala fact-checking system."""
import warnings

# Avoid noisy warnings in the UI logs on Python 3.14+
warnings.filterwarnings(
    "ignore",
    message=r"Core Pydantic V1 functionality isn't compatible with Python 3\.14 or greater\.",
)

import streamlit as st
import threading
from datetime import datetime
# noinspection PyUnresolvedReference
from google import genai
import os
from dotenv import load_dotenv

from src.vector_store import QdrantVectorStore
from src.search import MultiSourceSearch
from src.workflow import FactCheckingWorkflow
from src.cache import SimpleCache
from src.gemini_router import GeminiRouter

# Load environment variables
load_dotenv()
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
QDRANT_PATH = os.getenv("QDRANT_PATH", "./qdrant_data")
_VECTOR_STORE_LOCK = threading.Lock()

# Initialize session state
@st.cache_resource(show_spinner=False)
def get_vector_store(storage_path: str) -> QdrantVectorStore:
    # Streamlit can execute scripts concurrently across sessions; local Qdrant storage
    # cannot be opened by multiple clients at the same time.
    with _VECTOR_STORE_LOCK:
        return QdrantVectorStore(storage_path)


if "vector_store" not in st.session_state:
    st.session_state.vector_store = get_vector_store(QDRANT_PATH)

if "search" not in st.session_state:
    st.session_state.search = MultiSourceSearch()

if "workflow" not in st.session_state:
    st.session_state.workflow = FactCheckingWorkflow(
        st.session_state.vector_store,
        st.session_state.search,
        client=client
    )

if "cache" not in st.session_state:
    st.session_state.cache = SimpleCache(ttl_hours=24)

if "router" not in st.session_state:
    st.session_state.router = GeminiRouter(client=client)

if "history" not in st.session_state:
    st.session_state.history = []

# Page configuration
st.set_page_config(
    page_title="‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω ‡∑É‡∂≠‡∑ä‚Äç‡∂∫ ‡∑É‡∑ô‡∑Ä‡∑î‡∂∏‡∑ä‡∂ö‡∂ª‡∑î",
    page_icon="üîç",
    layout="wide"
)

# Title and description
st.title("üîç ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω ‡∑É‡∂≠‡∑ä‚Äç‡∂∫ ‡∑É‡∑ô‡∑Ä‡∑î‡∂∏‡∑ä‡∂ö‡∂ª‡∑î")
st.subheader("Sinhala Fact-Checking System")

# Sidebar with system info
with st.sidebar:
    st.header("üìä System Status")
    
    # Search quota
    quota = st.session_state.search.get_quota_status()
    st.subheader("Search Quota")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric(
            "Tavily",
            f"{quota['tavily']['remaining']}/1000",
            delta=f"-{quota['tavily']['used']}"
        )
    with col2:
        st.metric(
            "Brave",
            f"{quota['brave']['remaining']}/2000",
            delta=f"-{quota['brave']['used']}"
        )
    with col3:
        st.metric("DuckDuckGo", "‚ôæÔ∏è")
    
    # Cache stats
    st.subheader("Cache")
    cache_stats = st.session_state.cache.get_stats()
    st.metric("Cached Results", cache_stats["size"])
    
    # Gemini router stats
    st.subheader("Gemini Models")
    router_stats = st.session_state.router.get_stats()
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric(
            "Flash",
            f"{router_stats['flash']['used']}/15",
            delta=f"remaining: {15-router_stats['flash']['used']}"
        )
    with col2:
        st.metric(
            "Pro",
            f"{router_stats['pro']['used']}/2",
            delta=f"remaining: {2-router_stats['pro']['used']}"
        )
    with col3:
        st.metric(
            "Thinking",
            f"{router_stats['thinking']['used']}/10",
            delta=f"remaining: {10-router_stats['thinking']['used']}"
        )
    
    # Collection stats
    st.subheader("Vector Store")
    politics_stats = st.session_state.vector_store.get_collection_stats("politics")
    economics_stats = st.session_state.vector_store.get_collection_stats("economics")
    health_stats = st.session_state.vector_store.get_collection_stats("health")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Politics", politics_stats["document_count"])
    with col2:
        st.metric("Economics", economics_stats["document_count"])
    with col3:
        st.metric("Health", health_stats["document_count"])

# Main input area
st.divider()
col1, col2 = st.columns([3, 1])

with col1:
    statement = st.text_area(
        "‡∂¥‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å‡∂∫/‡∂≥‡∑î‡∂± (Enter a claim in Sinhala or English)",
        height=100,
        placeholder="‡∂ã‡∂Ø‡∑è: ‡∑Å‡∑ä‚Äç‡∂ª‡∑ì ‡∂Ω‡∂Ç‡∂ö‡∑è‡∑Ä‡∑ö ‡∂¢‡∑í‡∂©‡∑ì‡∂¥‡∑ì ‡∂ë‡∂ö‡∑ä‡∑É‡∂≠‡∑ä ‡∂¢‡∂±‡∂¥‡∂Ø ‡∂©‡∑ú‡∂Ω‡∂ª‡∑ä ‡∂∂‡∑í‡∂Ω‡∑í‡∂∫‡∂±‡∂∫‡∑í"
    )

with col2:
    st.write("")
    st.write("")
    verify_button = st.button("‚úì ‡∑É‡∂≠‡∑ä‚Äç‡∂∫‡∑è‡∂¥‡∂±‡∂∫", width="stretch")

# Verification logic
if verify_button and statement:
    # Check cache first
    cached_result = st.session_state.cache.get(statement)
    
    if cached_result:
        st.info("üéØ Cache hit - Using cached result")
        result = cached_result
        cached = True
    else:
        # Verify claim
        with st.spinner("‡∂¥‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∑è ‡∂ö‡∂ª‡∂∏‡∑í‡∂±‡∑ä... (Checking...)"):
            try:
                result = st.session_state.workflow.verify(statement)
                st.session_state.cache.set(statement, result)
                cached = False
            except (RuntimeError, ValueError) as e:
                st.error(f"Error during verification: {str(e)}")
                st.stop()
    
    # Display results
    st.divider()
    
    # Verdict
    verdict = result.get("verdict", "unknown")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        if verdict == "true":
            st.success("‚úÖ **‡∑É‡∂≠‡∑ä‚Äç‡∂∫‡∂∫‡∑í** (TRUE)")
        elif verdict == "false":
            st.error("‚ùå **‡∂Ö‡∑É‡∂≠‡∑ä‚Äç‡∂∫‡∂∫‡∑í** (FALSE)")
        else:
            st.warning("‚ö†Ô∏è **‡∂≠‡∑ú‡∂ª‡∂≠‡∑î‡∂ª‡∑î ‡∂¥‡∑ä‚Äç‡∂ª‡∂∏‡∑è‡∂´‡∑Ä‡∂≠‡∑ä ‡∂±‡∑ú‡∑Ä‡∑ö** (INSUFFICIENT)")
    
    # Analysis
    st.subheader("üìã ‡∑Ä‡∑í‡∑Å‡∑ä‡∂Ω‡∑ö‡∑Ç‡∂´ (Analysis)")
    st.write(result.get("analysis", "No analysis available"))
    
    # Details
    with st.expander("üìù ‡∑Ä‡∑í‡∑É‡∑ä‡∂≠‡∂ª (Details)"):
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("‡∂ö‡∑ä‡∑Ç‡∑ö‡∂≠‡∑ä‚Äç‡∂ª (Domain)", result.get("domain", "N/A"))
        with col2:
            st.metric("‡∂ö‡∑ä‚Äç‡∂ª‡∂∏‡∂∫ (Method)", result.get("search_source", "local"))
        with col3:
            st.metric("Cache Hit", "‚úì" if cached else "‚úó")
        with col4:
            st.metric("Timestamp", datetime.now().strftime("%H:%M:%S"))
        
        st.subheader("‡∂Ω‡∂∂‡∑è ‡∂ú‡∂≠‡∑ä ·û©‡∂¥‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å (Retrieved Documents)")
        for i, doc in enumerate(result.get("retrieved_docs", [])[:3]):
            st.write(f"**{i+1}. {doc.get('source', 'Unknown')}** (Score: {doc.get('score', 0):.2f})")
            st.text(doc.get("text", "")[:200] + "...")
    
    # Add to history
    st.session_state.history.append({
        "timestamp": datetime.now(),
        "statement": statement,
        "verdict": verdict,
        "cached": cached
    })

# History section
if st.session_state.history:
    st.divider()
    st.subheader("üìö ‡∂â‡∂≠‡∑í‡∑Ñ‡∑è‡∑É‡∂∫ (History)")
    
    history_df = []
    for item in st.session_state.history[-10:]:  # Last 10
        history_df.append({
            "‡∑É‡™Æ‡∂∫‡∂∫": item["timestamp"].strftime("%H:%M:%S"),
            "‡∂¥‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å‡∂∫": item["statement"][:50] + "...",
            "‡∂±‡∑í‡∂ú‡∂∏‡∂±‡∂∫": item["verdict"],
            "Cache": "‚úì" if item["cached"] else "‚úó"
        })
    
    if history_df:
        st.dataframe(history_df, width="stretch")
